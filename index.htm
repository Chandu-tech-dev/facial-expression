<!DOCTYPE html>
<html lang="en">

<head>
    <meta charset="UTF-8" />
    <title>Mood Music Wall</title>
    <script defer src="https://cdn.jsdelivr.net/npm/face-api.js"></script>
    <style>
        body {
            font-family: Arial, sans-serif;
            background: linear-gradient(#c6e2ff, #f2f2f2);
            text-align: center;
            padding: 20px;
        }

        video {
            width: 400px;
            height: 300px;
            border: 3px solid #333;
            border-radius: 10px;
        }

        #mood {
            font-size: 26px;
            margin-top: 20px;
            color: #004080;
            font-weight: bold;
        }

        audio {
            margin-top: 20px;
        }
    </style>
</head>

<body>
    <h2>ðŸŽµ Mood Music Wall</h2>
    <video id="video" autoplay muted playsinline></video>
    <div id="mood">Detecting mood...</div>
    <audio id="music" controls autoplay></audio>

    <script>
        const video = document.getElementById("video");
        const moodDisplay = document.getElementById("mood");
        const music = document.getElementById("music");

        const musicTracks = {
            happy: "https://www.soundhelix.com/examples/mp3/SoundHelix-Song-1.mp3",
            sad: "https://www.soundhelix.com/examples/mp3/SoundHelix-Song-2.mp3",
            angry: "https://www.soundhelix.com/examples/mp3/SoundHelix-Song-3.mp3",
            surprised: "https://www.soundhelix.com/examples/mp3/SoundHelix-Song-4.mp3",
            neutral: "https://www.soundhelix.com/examples/mp3/SoundHelix-Song-5.mp3"
        };

        let lastMood = "";

        async function setup() {
            await Promise.all([
                faceapi.nets.tinyFaceDetector.loadFromUri("https://cdn.jsdelivr.net/npm/face-api.js/models"),
                faceapi.nets.faceExpressionNet.loadFromUri("https://cdn.jsdelivr.net/npm/face-api.js/models")
            ]);

            try {
                const stream = await navigator.mediaDevices.getUserMedia({ video: true });
                video.srcObject = stream;
                video.onloadedmetadata = () => {
                    video.play();
                    detectMoodLoop();
                };
            } catch (err) {
                alert("Camera access denied or not available.");
                console.error(err);
            }
        }

        async function detectMoodLoop() {
            const canvas = faceapi.createCanvasFromMedia(video);
            document.body.append(canvas);

            const size = { width: video.width, height: video.height };
            faceapi.matchDimensions(canvas, size);

            setInterval(async () => {
                const detections = await faceapi
                    .detectAllFaces(video, new faceapi.TinyFaceDetectorOptions())
                    .withFaceExpressions();

                const resized = faceapi.resizeResults(detections, size);
                canvas.getContext("2d").clearRect(0, 0, canvas.width, canvas.height);
                faceapi.draw.drawDetections(canvas, resized);

                if (resized.length > 0) {
                    const expressions = resized[0].expressions;
                    const mood = Object.keys(expressions).reduce((a, b) =>
                        expressions[a] > expressions[b] ? a : b
                    );

                    moodDisplay.textContent=mood;

                    if (mood !== lastMood && musicTracks[mood]) {
                        music.src = musicTracks[mood];
                        lastMood = mood;
                    }
                } else {
                    moodDisplay.textContent = "No face detected";
                }
            }, 3000);
        }

        setup();
    </script>
</body>

</html>